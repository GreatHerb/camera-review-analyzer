"""
datapipe/crawl_youtube_comments.py

ì‚¬ìš© ì˜ˆ:
  cd datapipe
  source .venv/bin/activate
  python crawl_youtube_comments.py --query "ì¹´ë©”ë¼ ë¦¬ë·°" --max-videos 5 --comments-per-video 50

ì„¤ëª…:
 - query: ìœ íŠœë¸Œ ê²€ìƒ‰ì–´ (í•œêµ­ì–´ í‚¤ì›Œë“œ)
 - max-videos: ê²€ìƒ‰í•´ì„œ ì²˜ë¦¬í•  ìµœëŒ€ ë¹„ë””ì˜¤ ìˆ˜
 - comments-per-video: ë¹„ë””ì˜¤ë‹¹ ê°€ì ¸ì˜¬ ëŒ“ê¸€ ìˆ˜(ìƒìœ„ ëŒ“ê¸€ ê¸°ì¤€)
 - DB: camera_reviews ë°ì´í„°ë² ì´ìŠ¤ì˜ review í…Œì´ë¸”ì— INSERT
"""

import os
import time
import argparse
import html
import re

from googleapiclient.discovery import build
from sqlalchemy import create_engine, text
from sqlalchemy.exc import SQLAlchemyError
from tqdm import tqdm

# ---- ì„¤ì • ----

YOUTUBE_API_KEY = os.environ.get("YOUTUBE_API_KEY")
if not YOUTUBE_API_KEY:
    raise RuntimeError("YOUTUBE_API_KEY í™˜ê²½ë³€ìˆ˜ë¥¼ ë¨¼ì € ì„¤ì •í•˜ì„¸ìš”.")

# DB URL: ê¸°ì¡´ê³¼ ë™ì¼
DB_URL = os.environ.get(
    "DATABASE_URL",
    "postgresql+psycopg2://devuser:devpass@localhost:5432/camera_reviews"
)
engine = create_engine(DB_URL, future=True)

# YouTube API í´ë¼ì´ì–¸íŠ¸
yt = build("youtube", "v3", developerKey=YOUTUBE_API_KEY, cache_discovery=False)


def clean_text(s: str) -> str:
    """ê°„ë‹¨ í…ìŠ¤íŠ¸ ì •ì œ: HTML ì—”í‹°í‹°, URL, ë©˜ì…˜ ì œê±° + ê³µë°± ì •ë¦¬"""
    if not s:
        return s
    s = html.unescape(s)
    s = re.sub(r"https?://\S+", " ", s)        # URL ì œê±°
    s = re.sub(r"@[A-Za-z0-9_]+", " ", s)      # ë©˜ì…˜ ì œê±°
    s = re.sub(r"\s+", " ", s).strip()
    return s


def search_videos(query: str, max_results: int = 20):
    """ê²€ìƒ‰ì–´ë¡œ ìœ íŠœë¸Œ ë¹„ë””ì˜¤ ê²€ìƒ‰ í›„ videoId ë¦¬ìŠ¤íŠ¸ ë°˜í™˜"""
    video_ids = []
    next_page_token = None

    while len(video_ids) < max_results:
        resp = yt.search().list(
            q=query,
            part="id",
            type="video",
            maxResults=min(50, max_results - len(video_ids)),
            pageToken=next_page_token,
            relevanceLanguage="ko"
        ).execute()

        for item in resp.get("items", []):
            vid = item["id"]["videoId"]
            video_ids.append(vid)

        next_page_token = resp.get("nextPageToken")
        if not next_page_token:
            break

        time.sleep(0.1)

    return video_ids


def fetch_comments_for_video(video_id: str, max_comments: int = 200):
    """
    ê° ë¹„ë””ì˜¤ì˜ top-level ëŒ“ê¸€ ìˆ˜ì§‘
    ë°˜í™˜: ë¦¬ìŠ¤íŠ¸ of dict { 'video_id','text','publishedAt' }
    """
    comments = []
    next_token = None
    fetched = 0

    while fetched < max_comments:
        try:
            resp = yt.commentThreads().list(
                part="snippet",
                videoId=video_id,
                pageToken=next_token,
                maxResults=min(100, max_comments - fetched),
                textFormat="plainText"
            ).execute()
        except Exception as e:
            print(f"[warn] commentThreads ì—ëŸ¬(video={video_id}):", e)
            break

        items = resp.get("items", [])
        if not items:
            break

        for it in items:
            s = it["snippet"]["topLevelComment"]["snippet"]
            text_raw = s.get("textDisplay", "")
            comment = {
                "video_id": video_id,
                "text": clean_text(text_raw),
                "publishedAt": s.get("publishedAt")
            }
            if comment["text"]:
                comments.append(comment)
                fetched += 1
                if fetched >= max_comments:
                    break

        next_token = resp.get("nextPageToken")
        if not next_token:
            break

        time.sleep(0.1)

    return comments


def insert_reviews(rows):
    """
    review í…Œì´ë¸”ì— INSERT
    ê°€ì •: review í…Œì´ë¸” ì»¬ëŸ¼
      - id (serial)
      - source (text)
      - rating (int, nullable)
      - content (text)
      - created_at (timestamp without time zone, default now())
      - sentiment_label (text, nullable)
      - sentiment_score (numeric, nullable)
      - sentiment_model (text, nullable)
    """
    if not rows:
        return 0

    sql = text("""
        INSERT INTO review (source, rating, content, created_at)
        VALUES (:source, :rating, :content, :created_at)
    """)

    inserted = 0
    with engine.begin() as conn:
        for r in rows:
            try:
                conn.execute(sql, {
                    "source": r["source"],
                    "rating": None,
                    "content": r["content"],
                    "created_at": r["created_at"],
                })
                inserted += 1
            except SQLAlchemyError as e:
                print("[warn] DB insert error:", e)
    return inserted


def main(args):
    print(f"ğŸ” ê²€ìƒ‰ì–´: {args.query}")
    print(f"   â†’ ìµœëŒ€ ë¹„ë””ì˜¤ {args.max_videos}ê°œ, ë¹„ë””ì˜¤ë‹¹ ëŒ“ê¸€ {args.comments_per_video}ê°œ ìˆ˜ì§‘ ì‹œë„")

    video_ids = search_videos(args.query, max_results=args.max_videos)
    print("   ê²€ìƒ‰ëœ ë¹„ë””ì˜¤ ìˆ˜:", len(video_ids))

    total_inserted = 0

    for vid in tqdm(video_ids, desc="videos"):
        comments = fetch_comments_for_video(vid, max_comments=args.comments_per_video)

        rows = []
        for c in comments:
            rows.append({
                "source": f"youtube:{vid}",
                "content": c["text"],
                # publishedAtëŠ” ISO8601 í˜•ì‹ì´ë¼ PostgreSQLì´ ê·¸ëŒ€ë¡œ íŒŒì‹± ê°€ëŠ¥
                "created_at": c["publishedAt"],
            })

        inserted = insert_reviews(rows)
        total_inserted += inserted
        time.sleep(0.2)  # rate limit ì™„í™”ìš©

    print(f"âœ… ì´ ì‚½ì…ëœ ë¦¬ë·° ê°œìˆ˜: {total_inserted}")


if __name__ == "__main__":
    ap = argparse.ArgumentParser()
    ap.add_argument("--query", required=True, help="ìœ íŠœë¸Œ ê²€ìƒ‰ì–´ (ì˜ˆ: 'ì¹´ë©”ë¼ ë¦¬ë·°')")
    ap.add_argument("--max-videos", type=int, default=10, help="ê²€ìƒ‰í•´ì„œ ì²˜ë¦¬í•  ìµœëŒ€ ë¹„ë””ì˜¤ ìˆ˜")
    ap.add_argument("--comments-per-video", type=int, default=100, help="ë¹„ë””ì˜¤ë‹¹ ìµœëŒ€ ëŒ“ê¸€ ìˆ˜")
    args = ap.parse_args()

    main(args)